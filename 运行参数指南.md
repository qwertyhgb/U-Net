# UNet 图像分割训练参数指南

## 📋 目录

- [快速开始](#快速开始)
- [参数详解](#参数详解)
- [使用场景](#使用场景)
- [常见问题](#常见问题)
- [性能优化](#性能优化)

---

## 🚀 快速开始

### 基础训练（默认参数）
```bash
python train.py
```

### 推荐配置（GPU训练）
```bash
python train.py --epochs 50 --batch-size 8 --learning-rate 1e-4 --amp
```

### 显存不足时的配置
```bash
python train.py --epochs 50 --batch-size 2 --scale 0.25 --amp --bilinear
```

---

## 📖 参数详解

### 1. 基础训练参数

#### `--epochs` / `-e`
**训练轮数**

- **默认值**: 5
- **类型**: 整数
- **建议值**:
  - 小数据集（<1000张）: 50-100轮
  - 中等数据集（1000-5000张）: 30-50轮
  - 大数据集（>5000张）: 20-30轮

**示例**:
```bash
python train.py --epochs 50
python train.py -e 100
```

**注意事项**:
- 轮数过少可能导致欠拟合（模型未充分学习）
- 轮数过多可能导致过拟合（模型记住训练数据，泛化能力差）
- 建议配合早停机制使用（代码已内置）

---

#### `--batch-size` / `-b`
**批次大小**

- **默认值**: 1
- **类型**: 整数
- **建议值**:
  - 4GB显存: 1-2
  - 8GB显存: 4-8
  - 16GB显存: 8-16
  - 24GB显存: 16-32

**示例**:
```bash
python train.py --batch-size 8
python train.py -b 4
```

**影响**:
- ✅ 较大批次: 训练更稳定，速度更快
- ❌ 较大批次: 显存占用高，可能导致OOM错误
- ✅ 较小批次: 显存占用低，适合小显存GPU
- ❌ 较小批次: 训练不稳定，速度较慢

---

#### `--learning-rate` / `-l`
**学习率**

- **默认值**: 1e-5 (0.00001)
- **类型**: 浮点数
- **建议范围**: 1e-6 到 1e-3

**示例**:
```bash
python train.py --learning-rate 1e-4
python train.py -l 0.0001
```

**选择指南**:
- **1e-3**: 快速收敛，但可能不稳定
- **1e-4**: 平衡选择，适合大多数场景 ⭐
- **1e-5**: 稳定但收敛慢，适合微调
- **1e-6**: 非常保守，适合精细调整

---

### 2. 数据相关参数

#### `--scale` / `-s`
**图像缩放比例**

- **默认值**: 0.5
- **类型**: 浮点数
- **范围**: 0.1 到 1.0

**示例**:
```bash
python train.py --scale 0.75  # 缩放到75%
python train.py -s 0.25       # 缩放到25%
```

**影响**:
- **1.0**: 原始尺寸，精度最高，显存占用最大
- **0.75**: 高精度，显存占用较大
- **0.5**: 平衡选择 ⭐
- **0.25**: 低精度，显存占用最小

**计算示例**:
- 原图 512×512，scale=0.5 → 256×256
- 原图 1024×1024，scale=0.25 → 256×256

---

#### `--validation` / `-v`
**验证集比例**

- **默认值**: 10.0 (表示10%)
- **类型**: 浮点数
- **范围**: 0-100

**示例**:
```bash
python train.py --validation 20  # 20%用于验证
python train.py -v 15            # 15%用于验证
```

**建议**:
- 小数据集（<500张）: 20-30%
- 中等数据集（500-2000张）: 15-20%
- 大数据集（>2000张）: 10-15%

---

#### `--classes` / `-c`
**分割类别数**

- **默认值**: 2
- **类型**: 整数

**示例**:
```bash
python train.py --classes 2   # 二分类（前景/背景）
python train.py -c 5          # 5类分割
```

**说明**:
- 类别数包括背景类
- 例如：器官分割（器官+背景）= 2类
- 例如：多器官分割（肝+肾+脾+背景）= 4类

---

### 3. 模型相关参数

#### `--load` / `-f`
**加载预训练模型**

- **默认值**: False（不加载）
- **类型**: 字符串（文件路径）

**示例**:
```bash
# 从检查点恢复训练
python train.py --load checkpoints/checkpoint_epoch10.pth

# 使用最佳模型继续训练
python train.py -f checkpoints/best_model.pth --epochs 50
```

**用途**:
- 断点续训（训练中断后恢复）
- 迁移学习（使用预训练权重）
- 微调模型（在新数据集上调整）

---

#### `--bilinear`
**使用双线性插值上采样**

- **默认值**: False（使用转置卷积）
- **类型**: 布尔标志

**示例**:
```bash
python train.py --bilinear
```

**对比**:

| 特性 | 转置卷积（默认） | 双线性插值 |
|------|-----------------|-----------|
| 参数量 | 多 | 少 |
| 显存占用 | 高 | 低 |
| 分割精度 | 高 ⭐ | 中 |
| 训练速度 | 慢 | 快 |

**建议**:
- 显存充足 → 使用转置卷积（默认）
- 显存不足 → 使用双线性插值

---

### 4. 性能优化参数

#### `--amp`
**混合精度训练**

- **默认值**: False
- **类型**: 布尔标志

**示例**:
```bash
python train.py --amp
```

**优势**:
- ✅ 显存占用减少约50%
- ✅ 训练速度提升30-50%
- ✅ 精度几乎无损失

**要求**:
- NVIDIA GPU（支持Tensor Core）
- PyTorch 1.6+
- CUDA 10.0+

**推荐**: 强烈建议在GPU训练时启用 ⭐⭐⭐

---

#### `--accumulate-grad-batches`
**梯度累积批次数**

- **默认值**: 1（不累积）
- **类型**: 整数

**示例**:
```bash
# 模拟batch_size=16的效果
python train.py --batch-size 4 --accumulate-grad-batches 4
```

**原理**:
- 实际batch_size = batch_size × accumulate_grad_batches
- 用时间换空间，适合显存不足的情况

**使用场景**:
```bash
# 显存只能支持batch_size=2，但想要batch_size=8的效果
python train.py --batch-size 2 --accumulate-grad-batches 4
```

---

## 🎯 使用场景

### 场景1: 初次训练（探索性）
```bash
python train.py \
  --epochs 10 \
  --batch-size 4 \
  --learning-rate 1e-4 \
  --validation 20 \
  --amp
```

**说明**: 快速验证数据和模型是否正常工作

---

### 场景2: 正式训练（高性能GPU）
```bash
python train.py \
  --epochs 100 \
  --batch-size 16 \
  --learning-rate 1e-4 \
  --scale 0.75 \
  --validation 15 \
  --amp
```

**说明**: 充分利用GPU性能，追求最佳效果

---

### 场景3: 显存不足（低端GPU）
```bash
python train.py \
  --epochs 80 \
  --batch-size 2 \
  --learning-rate 1e-4 \
  --scale 0.25 \
  --validation 15 \
  --amp \
  --bilinear \
  --accumulate-grad-batches 4
```

**说明**: 通过多种优化手段降低显存占用

---

### 场景4: 从检查点恢复训练
```bash
python train.py \
  --load checkpoints/checkpoint_epoch30.pth \
  --epochs 100 \
  --batch-size 8 \
  --learning-rate 1e-5 \
  --amp
```

**说明**: 训练中断后继续，注意学习率可能需要调小

---

### 场景5: 多分类分割任务
```bash
python train.py \
  --epochs 80 \
  --batch-size 8 \
  --learning-rate 1e-4 \
  --classes 5 \
  --scale 0.5 \
  --validation 15 \
  --amp
```

**说明**: 5类分割任务（例如：4个器官+背景）

---

### 场景6: 高精度训练
```bash
python train.py \
  --epochs 150 \
  --batch-size 4 \
  --learning-rate 5e-5 \
  --scale 1.0 \
  --validation 10 \
  --amp
```

**说明**: 使用原始图像尺寸，追求最高精度

---

## ❓ 常见问题

### Q1: 显存溢出（CUDA Out of Memory）

**错误信息**:
```
RuntimeError: CUDA out of memory
```

**解决方案**（按优先级）:
1. 启用混合精度训练: `--amp`
2. 减小批次大小: `--batch-size 2` 或 `--batch-size 1`
3. 减小图像尺寸: `--scale 0.25`
4. 使用双线性上采样: `--bilinear`
5. 使用梯度累积: `--accumulate-grad-batches 4`

**组合方案**:
```bash
python train.py --batch-size 1 --scale 0.25 --amp --bilinear
```

---

### Q2: 训练速度太慢

**可能原因**:
- CPU训练（未使用GPU）
- 批次大小太小
- 未启用混合精度训练
- 数据加载瓶颈

**解决方案**:
```bash
# 确保使用GPU + 混合精度 + 合理批次大小
python train.py --batch-size 8 --amp
```

---

### Q3: 模型不收敛（损失不下降）

**可能原因**:
- 学习率过大或过小
- 数据标注有问题
- 模型架构不适合

**解决方案**:
1. 调整学习率:
```bash
# 尝试不同学习率
python train.py --learning-rate 1e-3  # 较大
python train.py --learning-rate 1e-5  # 较小
```

2. 检查数据:
- 确认图像和掩码对应正确
- 检查掩码标签值范围（0到n_classes-1）

---

### Q4: 过拟合（训练集好，验证集差）

**症状**:
- 训练损失持续下降
- 验证Dice分数不提升或下降

**解决方案**:
1. 增加验证集比例: `--validation 25`
2. 减少训练轮数（早停会自动处理）
3. 增加数据增强（需修改代码）
4. 使用更多训练数据

---

### Q5: 如何选择最佳模型？

**答案**: 代码会自动保存最佳模型

- 每轮训练后保存: `checkpoints/checkpoint_epoch{N}.pth`
- 最佳模型保存: `checkpoints/best_model.pth` ⭐

**使用最佳模型**:
```bash
# 推理时使用
python predict.py --model checkpoints/best_model.pth

# 继续训练
python train.py --load checkpoints/best_model.pth
```

---

## ⚡ 性能优化建议

### 显存优化优先级

1. **启用混合精度** (最有效) ⭐⭐⭐
```bash
--amp
```

2. **减小批次大小**
```bash
--batch-size 2
```

3. **减小图像尺寸**
```bash
--scale 0.25
```

4. **使用双线性上采样**
```bash
--bilinear
```

5. **梯度累积**
```bash
--accumulate-grad-batches 4
```

---

### 训练速度优化

1. **使用GPU** (必须)
2. **启用混合精度** `--amp`
3. **增大批次大小** `--batch-size 16`
4. **减小图像尺寸** `--scale 0.5`
5. **使用SSD存储数据**

---

### 精度优化

1. **使用原始图像尺寸** `--scale 1.0`
2. **增加训练轮数** `--epochs 100`
3. **使用转置卷积**（默认，不加`--bilinear`）
4. **调整学习率** `--learning-rate 1e-4`
5. **增加训练数据**

---

## 📊 参数组合推荐

### 配置1: 快速测试
```bash
python train.py --epochs 5 --batch-size 4 --amp
```
**用途**: 验证代码和数据是否正常

---

### 配置2: 标准训练（推荐）⭐
```bash
python train.py \
  --epochs 50 \
  --batch-size 8 \
  --learning-rate 1e-4 \
  --scale 0.5 \
  --validation 15 \
  --amp
```
**用途**: 大多数场景的默认选择

---

### 配置3: 高精度训练
```bash
python train.py \
  --epochs 100 \
  --batch-size 4 \
  --learning-rate 5e-5 \
  --scale 1.0 \
  --validation 10 \
  --amp
```
**用途**: 追求最佳分割效果

---

### 配置4: 低显存训练
```bash
python train.py \
  --epochs 80 \
  --batch-size 1 \
  --learning-rate 1e-4 \
  --scale 0.25 \
  --validation 15 \
  --amp \
  --bilinear \
  --accumulate-grad-batches 8
```
**用途**: 4GB显存或更少

---

## 📝 训练监控

### WandB可视化

训练过程会自动记录到WandB平台，可以实时查看：

- 训练损失曲线
- 验证Dice分数
- 学习率变化
- 权重和梯度分布
- 预测结果可视化

**访问**: 训练开始后会显示WandB链接

---

### 检查点文件

**保存位置**: `checkpoints/`

**文件说明**:
- `checkpoint_epoch{N}.pth`: 每轮训练后的检查点
- `best_model.pth`: 验证集上表现最好的模型 ⭐

**检查点内容**:
- 模型权重
- 优化器状态
- 学习率调度器状态
- 训练轮次
- 损失值
- 验证分数

---

## 🔧 故障排除

### 问题: 找不到数据

**错误**: `FileNotFoundError` 或 `数据集为空`

**检查**:
1. 数据目录结构:
```
data/
├── imgs/          # 训练图像
│   ├── img1.jpg
│   ├── img2.jpg
│   └── ...
└── masks/         # 标注掩码
    ├── img1.png
    ├── img2.png
    └── ...
```

2. 图像和掩码文件名必须对应

---

### 问题: 训练中断

**解决**: 从检查点恢复
```bash
python train.py --load checkpoints/checkpoint_epoch20.pth --epochs 50
```

---

### 问题: 验证分数为0或很低

**可能原因**:
1. 掩码标签值不正确
2. 类别数设置错误
3. 数据预处理问题

**检查**:
- 掩码值应该是 0, 1, 2, ..., n_classes-1
- `--classes` 参数应该等于实际类别数

---

## 📚 更多资源

- **项目文档**: README.md
- **模型架构**: unet.py
- **数据加载**: utils/data_loading.py
- **评估指标**: evaluate.py

---

## 💡 最佳实践总结

1. ✅ **始终使用混合精度训练** (`--amp`)
2. ✅ **根据显存调整批次大小**
3. ✅ **从小规模实验开始** (少轮数快速验证)
4. ✅ **监控训练过程** (WandB可视化)
5. ✅ **使用早停机制** (代码已内置)
6. ✅ **保存检查点** (默认启用)
7. ✅ **验证数据质量** (训练前检查)
8. ✅ **合理设置验证集比例** (10-20%)

---

**祝训练顺利！** 🎉

如有问题，请查看代码注释或提交Issue。
