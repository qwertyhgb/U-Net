
"""
UNet å›¾åƒåˆ†å‰²æ¨¡åž‹è®­ç»ƒè„šæœ¬

è¯¥è„šæœ¬å®žçŽ°äº†å®Œæ•´çš„UNetæ¨¡åž‹è®­ç»ƒæµç¨‹ï¼Œä¸“é—¨ç”¨äºŽåŒ»å­¦å›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚
UNetæ˜¯ä¸€ç§ç¼–ç å™¨-è§£ç å™¨æž¶æž„çš„å·ç§¯ç¥žç»ç½‘ç»œï¼Œåœ¨å›¾åƒåˆ†å‰²é¢†åŸŸè¡¨çŽ°ä¼˜å¼‚ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
1. æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†ï¼ˆæ”¯æŒå¤šç§å›¾åƒæ ¼å¼ï¼‰
2. è®­ç»ƒ/éªŒè¯é›†è‡ªåŠ¨åˆ’åˆ†
3. UNetæ¨¡åž‹è®­ç»ƒï¼ˆæ”¯æŒäºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ï¼‰
4. æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰åŠ é€Ÿå’Œæ˜¾å­˜ä¼˜åŒ–
5. å®žæ—¶æ€§èƒ½ç›‘æŽ§å’Œå¯è§†åŒ–ï¼ˆWandBé›†æˆï¼‰
6. è‡ªåŠ¨ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹å’Œæ¨¡åž‹æ¢å¤
7. æ¢¯åº¦è£å‰ªå’Œå­¦ä¹ çŽ‡è°ƒåº¦
8. å¼‚å¸¸å¤„ç†å’Œé”™è¯¯æ¢å¤æœºåˆ¶

è®­ç»ƒç®—æ³•ç‰¹ç‚¹ï¼š
- æŸå¤±å‡½æ•°ï¼šäº¤å‰ç†µ + DiceæŸå¤±ï¼ˆå¤šåˆ†ç±»ï¼‰/ BCE + DiceæŸå¤±ï¼ˆäºŒåˆ†ç±»ï¼‰
- ä¼˜åŒ–å™¨ï¼šAdamWï¼ˆå¸¦æƒé‡è¡°å‡å’ŒAMSGradï¼‰
- å­¦ä¹ çŽ‡è°ƒåº¦ï¼šä½™å¼¦é€€ç«é‡å¯ï¼ˆCosineAnnealingWarmRestartsï¼‰
- æ•°æ®å¢žå¼ºï¼šå›¾åƒç¼©æ”¾ã€éšæœºè£å‰ªç­‰
- æ­£åˆ™åŒ–ï¼šL2æƒé‡è¡°å‡ã€æ¢¯åº¦è£å‰ª

æŠ€æœ¯äº®ç‚¹ï¼š
1. è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰å‡å°‘æ˜¾å­˜å ç”¨50%
2. é€šé“ä¼˜å…ˆå†…å­˜æ ¼å¼ä¼˜åŒ–GPUæ€§èƒ½
3. æ¢¯åº¦æ£€æŸ¥ç‚¹æœºåˆ¶å¤„ç†å¤§æ¨¡åž‹æ˜¾å­˜ä¸è¶³
4. å®žæ—¶è®­ç»ƒç›‘æŽ§å’Œå¯è§†åŒ–
5. æ™ºèƒ½å¼‚å¸¸å¤„ç†å’Œè‡ªåŠ¨æ¢å¤

é€‚ç”¨åœºæ™¯ï¼š
- åŒ»å­¦å›¾åƒåˆ†å‰²ï¼ˆå™¨å®˜ã€ç—…å˜åŒºåŸŸç­‰ï¼‰
- å«æ˜Ÿå›¾åƒåˆ†å‰²ï¼ˆå»ºç­‘ã€é“è·¯ç­‰ï¼‰
- ç”Ÿç‰©å›¾åƒåˆ†æžï¼ˆç»†èƒžåˆ†å‰²ç­‰ï¼‰
- è‡ªåŠ¨é©¾é©¶åœºæ™¯åˆ†å‰²

ä½¿ç”¨ç¤ºä¾‹ï¼š
    # åŸºç¡€è®­ç»ƒ
    python train.py --epochs 50 --batch-size 8 --learning-rate 1e-4
    
    # é«˜æ€§èƒ½è®­ç»ƒï¼ˆæ··åˆç²¾åº¦+å¤§æ‰¹æ¬¡ï¼‰
    python train.py --epochs 100 --batch-size 16 --amp --scale 0.75
    
    # ä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    python train.py --load checkpoints/checkpoint_epoch10.pth --epochs 50
"""

# ================================
# æ ‡å‡†åº“å¯¼å…¥
# ================================
import argparse  # ç”¨äºŽè§£æžå‘½ä»¤è¡Œå‚æ•°ï¼Œæ”¯æŒå¤æ‚çš„å‚æ•°é…ç½®
import logging   # ç”¨äºŽè®°å½•è®­ç»ƒæ—¥å¿—ï¼Œæ”¯æŒä¸åŒçº§åˆ«çš„æ—¥å¿—è¾“å‡º
import os        # å¤„ç†æ“ä½œç³»ç»Ÿç›¸å…³æ“ä½œï¼Œå¦‚æ–‡ä»¶è·¯å¾„ã€çŽ¯å¢ƒå˜é‡ç­‰
import random    # éšæœºæ•°ç”Ÿæˆï¼Œç”¨äºŽæ•°æ®å¢žå¼ºå’Œåˆå§‹åŒ–
import sys       # ç³»ç»Ÿç›¸å…³åŠŸèƒ½ï¼Œå¦‚ç¨‹åºé€€å‡ºã€å¼‚å¸¸å¤„ç†ç­‰

# ================================
# æ·±åº¦å­¦ä¹ ç›¸å…³åº“å¯¼å…¥
# ================================
import torch            # PyTorchæ·±åº¦å­¦ä¹ æ¡†æž¶æ ¸å¿ƒåº“
import torch.nn as nn   # ç¥žç»ç½‘ç»œæ¨¡å—ï¼ŒåŒ…å«å„ç§å±‚å’Œæ¿€æ´»å‡½æ•°
import torch.nn.functional as F  # ç¥žç»ç½‘ç»œå‡½æ•°åº“ï¼Œæä¾›æ— çŠ¶æ€çš„å‡½æ•°å¼æŽ¥å£
import torchvision.transforms as transforms        # å›¾åƒå˜æ¢å·¥å…·ï¼Œç”¨äºŽæ•°æ®é¢„å¤„ç†
import torchvision.transforms.functional as TF     # å›¾åƒå˜æ¢å‡½æ•°ï¼Œæä¾›åº•å±‚å˜æ¢æ“ä½œ
from torch import optim   # ä¼˜åŒ–å™¨æ¨¡å—ï¼ŒåŒ…å«å„ç§æ¢¯åº¦ä¸‹é™ç®—æ³•
from torch.utils.data import DataLoader, random_split  # æ•°æ®åŠ è½½å’Œåˆ’åˆ†å·¥å…·

# ================================
# å·¥å…·åº“å¯¼å…¥
# ================================
from pathlib import Path  # è·¯å¾„å¤„ç†å·¥å…·ï¼Œæä¾›è·¨å¹³å°è·¯å¾„æ“ä½œ
from tqdm import tqdm     # è¿›åº¦æ¡æ˜¾ç¤ºåº“ï¼Œæä¾›ç¾Žè§‚çš„è®­ç»ƒè¿›åº¦å¯è§†åŒ–

# ================================
# é¡¹ç›®ç›¸å…³å¯¼å…¥
# ================================
import wandb  # Weights & Biaseså®žéªŒç®¡ç†å¹³å°ï¼Œç”¨äºŽè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å’Œè¶…å‚æ•°è°ƒä¼˜
from evaluate import evaluate  # æ¨¡åž‹éªŒè¯è¯„ä¼°å‡½æ•°ï¼Œè®¡ç®—Diceç³»æ•°ç­‰æŒ‡æ ‡
from unet import UNet         # UNetæ¨¡åž‹æž¶æž„å®šä¹‰ï¼Œç¼–ç å™¨-è§£ç å™¨åˆ†å‰²ç½‘ç»œ
from utils.data_loading import BasicDataset, CarvanaDataset  # æ•°æ®é›†åŠ è½½ç±»ï¼Œæ”¯æŒå¤šç§æ•°æ®æ ¼å¼
from utils.dice_score import dice_loss  # DiceæŸå¤±å‡½æ•°ï¼Œä¸“é—¨ç”¨äºŽåˆ†å‰²ä»»åŠ¡çš„æŸå¤±è®¡ç®—


# ================================
# å…¨å±€é…ç½®ç±»
# ================================
class Config:
    """
    å…¨å±€é…ç½®ç±»ï¼Œé›†ä¸­ç®¡ç†é¡¹ç›®é…ç½®å‚æ•°
    
    è¯¥ç±»é‡‡ç”¨é›†ä¸­å¼é…ç½®ç®¡ç†ï¼Œä¾¿äºŽå‚æ•°è°ƒä¼˜å’Œç»´æŠ¤ã€‚
    æ‰€æœ‰é…ç½®å‚æ•°éƒ½å®šä¹‰ä¸ºç±»å±žæ€§ï¼Œå¯ä»¥åœ¨æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­ç»Ÿä¸€è®¿é—®ã€‚
    
    é…ç½®åˆ†ç±»ï¼š
    1. æ•°æ®è·¯å¾„é…ç½®ï¼šå®šä¹‰æ•°æ®é›†ã€æ£€æŸ¥ç‚¹çš„å­˜å‚¨ä½ç½®
    2. è®­ç»ƒå‚æ•°é…ç½®ï¼šæŽ§åˆ¶è®­ç»ƒè¿‡ç¨‹çš„è¶…å‚æ•°
    3. ç¡¬ä»¶èµ„æºé…ç½®ï¼šä¼˜åŒ–å¤šæ ¸CPUå’ŒGPUä½¿ç”¨æ•ˆçŽ‡
    
    è®¾è®¡åŽŸåˆ™ï¼š
    - å•ä¸€èŒè´£ï¼šæ¯ä¸ªé…ç½®é¡¹éƒ½æœ‰æ˜Žç¡®çš„ä½œç”¨
    - å¯æ‰©å±•æ€§ï¼šæ˜“äºŽæ·»åŠ æ–°çš„é…ç½®å‚æ•°
    - å¯ç»´æŠ¤æ€§ï¼šé›†ä¸­ç®¡ç†ï¼Œä¾¿äºŽæ‰¹é‡ä¿®æ”¹
    """
    
    # ================================
    # æ•°æ®ç›¸å…³è·¯å¾„é…ç½®
    # ================================
    DATA_DIR = Path('./data')          # æ•°æ®æ ¹ç›®å½•ï¼Œå­˜å‚¨æ‰€æœ‰è®­ç»ƒæ•°æ®
    IMG_DIR = DATA_DIR / 'imgs'        # è®­ç»ƒå›¾ç‰‡å­˜å‚¨è·¯å¾„ï¼Œæ”¯æŒJPGã€PNGç­‰æ ¼å¼
    MASK_DIR = DATA_DIR / 'masks'      # æ ‡æ³¨æŽ©ç å­˜å‚¨è·¯å¾„ï¼Œä¸Žå›¾ç‰‡ä¸€ä¸€å¯¹åº”
    CHECKPOINT_DIR = Path('./checkpoints')  # æ¨¡åž‹æ£€æŸ¥ç‚¹ä¿å­˜è·¯å¾„ï¼Œç”¨äºŽæ–­ç‚¹ç»­è®­
    
    # ================================
    # è®­ç»ƒç›¸å…³å‚æ•°é…ç½®
    # ================================
    RANDOM_SEED = 42        # éšæœºæ•°ç§å­ï¼Œç¡®ä¿å®žéªŒå¯é‡å¤æ€§
    VAL_INTERVAL = 0.2     # éªŒè¯é—´éš”æ¯”ä¾‹ï¼ˆæ¯è®­ç»ƒå¤šå°‘è½®è¿›è¡Œä¸€æ¬¡éªŒè¯ï¼‰
    
    # ================================
    # ç¡¬ä»¶èµ„æºç›¸å…³é…ç½®
    # ================================
    NUM_WORKERS = min(os.cpu_count(), 8)  # æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼Œé¿å…CPUè¿‡è½½
    PIN_MEMORY = torch.cuda.is_available()  # GPUè®­ç»ƒæ—¶å¯ç”¨é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿæ•°æ®ä¼ è¾“
    PREFETCH_FACTOR = 2  # é¢„åŠ è½½æ‰¹æ¬¡æ•°ï¼Œå¹³è¡¡å†…å­˜å ç”¨å’ŒåŠ è½½é€Ÿåº¦


def train_model(
        model: torch.nn.Module,
        device: torch.device,
        epochs: int = 5,
        batch_size: int = 1,
        learning_rate: float = 1e-5,
        val_percent: float = 0.1,
        save_checkpoint: bool = True,
        img_scale: float = 0.5,
        amp: bool = False,
        weight_decay: float = 1e-8,
        momentum: float = 0.999,
        gradient_clipping: float = 1.0,
):
    """
    UNetæ¨¡åž‹è®­ç»ƒçš„ä¸»å‡½æ•°ï¼Œå®žçŽ°å®Œæ•´çš„è®­ç»ƒæµç¨‹

    è¯¥å‡½æ•°æ˜¯è®­ç»ƒè„šæœ¬çš„æ ¸å¿ƒï¼Œå®žçŽ°äº†ä»Žæ•°æ®åŠ è½½åˆ°æ¨¡åž‹ä¿å­˜çš„å®Œæ•´è®­ç»ƒæµç¨‹ã€‚
    æ”¯æŒå¤šç§è®­ç»ƒç­–ç•¥å’Œä¼˜åŒ–æŠ€æœ¯ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ•ˆçŽ‡ã€‚
    
    ================================
    è®­ç»ƒæµç¨‹æ¦‚è¿°ï¼š
    ================================
    1. æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†
       - è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨åˆé€‚çš„æ•°æ®é›†ç±»ï¼ˆCarvanaDatasetæˆ–BasicDatasetï¼‰
       - æŒ‰æ¯”ä¾‹åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
       - é…ç½®é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨
    
    2. è®­ç»ƒçŽ¯å¢ƒåˆå§‹åŒ–
       - åˆå§‹åŒ–WandBå®žéªŒè·Ÿè¸ª
       - é…ç½®ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°å’Œå­¦ä¹ çŽ‡è°ƒåº¦å™¨
       - è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰çŽ¯å¢ƒ
    
    3. è®­ç»ƒå¾ªçŽ¯æ‰§è¡Œ
       - å‰å‘ä¼ æ’­ï¼šè®¡ç®—é¢„æµ‹ç»“æžœå’ŒæŸå¤±
       - åå‘ä¼ æ’­ï¼šè®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°æ¨¡åž‹å‚æ•°
       - å®šæœŸéªŒè¯ï¼šè¯„ä¼°æ¨¡åž‹æ€§èƒ½å¹¶è°ƒæ•´å­¦ä¹ çŽ‡
       - å®žæ—¶ç›‘æŽ§ï¼šè®°å½•è®­ç»ƒæŒ‡æ ‡å’Œå¯è§†åŒ–ç»“æžœ
    
    4. æ¨¡åž‹ä¿å­˜å’Œæ¢å¤
       - è‡ªåŠ¨ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹
       - æ”¯æŒä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    
    ================================
    å‚æ•°è¯¦ç»†è¯´æ˜Žï¼š
    ================================
    Args:
        model (torch.nn.Module): å¾…è®­ç»ƒçš„UNetæ¨¡åž‹å®žä¾‹
            - å¿…é¡»æ˜¯UNetç±»çš„å®žä¾‹ï¼ŒåŒ…å«n_channelså’Œn_classeså±žæ€§
            - æ¨¡åž‹åº”è¯¥å·²ç»ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ä¸Š
            - æ”¯æŒé¢„è®­ç»ƒæƒé‡åŠ è½½
        
        device (torch.device): è®­ç»ƒè®¾å¤‡
            - 'cuda': ä½¿ç”¨GPUè®­ç»ƒï¼Œéœ€è¦CUDAæ”¯æŒ
            - 'cpu': ä½¿ç”¨CPUè®­ç»ƒï¼Œé€Ÿåº¦è¾ƒæ…¢ä½†å…¼å®¹æ€§å¥½
            - 'mps': Apple Siliconè®¾å¤‡ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        
        epochs (int, optional): æ€»è®­ç»ƒè½®æ•°. é»˜è®¤å€¼: 5
            - æ¯è½®éåŽ†æ•´ä¸ªè®­ç»ƒé›†ä¸€æ¬¡
            - å»ºè®®å€¼ï¼šå°æ•°æ®é›†50-100è½®ï¼Œå¤§æ•°æ®é›†20-50è½®
            - è¿‡å°‘å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆï¼Œè¿‡å¤šå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ
        
        batch_size (int, optional): æ‰¹æ¬¡å¤§å°. é»˜è®¤å€¼: 1
            - æ¯æ‰¹æ¬¡å¤„ç†çš„æ ·æœ¬æ•°ï¼Œç›´æŽ¥å½±å“è®­ç»ƒé€Ÿåº¦å’Œæ˜¾å­˜å ç”¨
            - GPUæ˜¾å­˜å»ºè®®ï¼š8GBæ˜¾å­˜å¯ç”¨batch_size=4-8ï¼Œ16GBå¯ç”¨8-16
            - è¿‡å°å½±å“è®­ç»ƒæ•ˆçŽ‡ï¼Œè¿‡å¤§å¯èƒ½å¯¼è‡´æ˜¾å­˜æº¢å‡º
        
        learning_rate (float, optional): åˆå§‹å­¦ä¹ çŽ‡. é»˜è®¤å€¼: 1e-5
            - æŽ§åˆ¶æ¨¡åž‹å‚æ•°æ›´æ–°çš„æ­¥é•¿
            - å»ºè®®èŒƒå›´ï¼š1e-4 åˆ° 1e-6
            - è¿‡é«˜å¯èƒ½å¯¼è‡´è®­ç»ƒä¸ç¨³å®šï¼Œè¿‡ä½Žå¯èƒ½å¯¼è‡´æ”¶æ•›ç¼“æ…¢
        
        val_percent (float, optional): éªŒè¯é›†æ¯”ä¾‹. é»˜è®¤å€¼: 0.1
            - éªŒè¯é›†å æ€»æ•°æ®é›†çš„æ¯”ä¾‹ï¼ŒèŒƒå›´[0, 1]
            - 0.1è¡¨ç¤º10%æ•°æ®ç”¨äºŽéªŒè¯ï¼Œ90%ç”¨äºŽè®­ç»ƒ
            - å»ºè®®å€¼ï¼šå°æ•°æ®é›†0.2-0.3ï¼Œå¤§æ•°æ®é›†0.1-0.2
        
        save_checkpoint (bool, optional): æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹. é»˜è®¤å€¼: True
            - True: æ¯è½®ç»“æŸåŽä¿å­˜æ¨¡åž‹çŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­
            - False: ä¸ä¿å­˜æ£€æŸ¥ç‚¹ï¼ŒèŠ‚çœç£ç›˜ç©ºé—´
        
        img_scale (float, optional): å›¾åƒç¼©æ”¾æ¯”ä¾‹. é»˜è®¤å€¼: 0.5
            - æŽ§åˆ¶è¾“å…¥å›¾åƒçš„å°ºå¯¸ï¼ŒèŒƒå›´(0, 1]
            - 0.5è¡¨ç¤ºå°†å›¾åƒå°ºå¯¸ç¼©å°åˆ°åŽŸæ¥çš„50%
            - è¾ƒå°çš„å€¼å¯ä»¥å‡å°‘æ˜¾å­˜å ç”¨ï¼Œä½†å¯èƒ½å½±å“åˆ†å‰²ç²¾åº¦
        
        amp (bool, optional): æ˜¯å¦å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ. é»˜è®¤å€¼: False
            - True: ä½¿ç”¨FP16ç²¾åº¦è®­ç»ƒï¼Œå¯å‡å°‘50%æ˜¾å­˜å ç”¨
            - False: ä½¿ç”¨FP32ç²¾åº¦è®­ç»ƒï¼Œæ•°å€¼ç¨³å®šæ€§æ›´å¥½
            - å»ºè®®åœ¨æ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨
        
        weight_decay (float, optional): L2æ­£åˆ™åŒ–ç³»æ•°. é»˜è®¤å€¼: 1e-8
            - é˜²æ­¢è¿‡æ‹Ÿåˆçš„æ­£åˆ™åŒ–é¡¹
            - å»ºè®®èŒƒå›´ï¼š1e-8 åˆ° 1e-4
            - è¿‡å¤§ä¼šå½±å“æ¨¡åž‹å­¦ä¹ èƒ½åŠ›ï¼Œè¿‡å°å¯èƒ½æ— æ³•é˜²æ­¢è¿‡æ‹Ÿåˆ
        
        momentum (float, optional): åŠ¨é‡å› å­. é»˜è®¤å€¼: 0.999
            - ä¼˜åŒ–å™¨çš„åŠ¨é‡å‚æ•°ï¼Œå½±å“å‚æ•°æ›´æ–°æ–¹å‘
            - ä»…åœ¨ä½¿ç”¨SGDä¼˜åŒ–å™¨æ—¶æœ‰æ•ˆ
            - å»ºè®®èŒƒå›´ï¼š0.9 åˆ° 0.999
        
        gradient_clipping (float, optional): æ¢¯åº¦è£å‰ªé˜ˆå€¼. é»˜è®¤å€¼: 1.0
            - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸çš„æŠ€æœ¯
            - å½“æ¢¯åº¦èŒƒæ•°è¶…è¿‡æ­¤å€¼æ—¶è¿›è¡Œè£å‰ª
            - å»ºè®®èŒƒå›´ï¼š0.5 åˆ° 2.0
    
    Returns:
        None: è¯¥å‡½æ•°ä¸è¿”å›žå€¼ï¼Œè®­ç»ƒç»“æžœé€šè¿‡æ£€æŸ¥ç‚¹æ–‡ä»¶å’ŒWandBè®°å½•
    
    Raises:
        RuntimeError: å½“æ•°æ®é›†åŠ è½½å¤±è´¥æ—¶æŠ›å‡º
        AssertionError: å½“æ¨¡åž‹è¾“å…¥é€šé“æ•°ä¸Žå›¾åƒé€šé“æ•°ä¸åŒ¹é…æ—¶æŠ›å‡º
        torch.cuda.OutOfMemoryError: å½“GPUæ˜¾å­˜ä¸è¶³æ—¶æŠ›å‡º
    
    ================================
    è®­ç»ƒç­–ç•¥è¯´æ˜Žï¼š
    ================================
    
    1. æŸå¤±å‡½æ•°ç»„åˆï¼š
       - äºŒåˆ†ç±»ä»»åŠ¡ï¼šBCEæŸå¤± + DiceæŸå¤±
       - å¤šåˆ†ç±»ä»»åŠ¡ï¼šäº¤å‰ç†µæŸå¤± + DiceæŸå¤±
       - DiceæŸå¤±ä¸“é—¨ä¼˜åŒ–åˆ†å‰²ä»»åŠ¡çš„é‡å åº¦æŒ‡æ ‡
    
    2. ä¼˜åŒ–å™¨é…ç½®ï¼š
       - ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œç»“åˆäº†Adamå’Œæƒé‡è¡°å‡çš„ä¼˜åŠ¿
       - å¯ç”¨AMSGradå˜ä½“ï¼Œæä¾›æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
       - è‡ªåŠ¨æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
    
    3. å­¦ä¹ çŽ‡è°ƒåº¦ï¼š
       - ä½¿ç”¨ä½™å¼¦é€€ç«é‡å¯ç­–ç•¥
       - åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ çŽ‡
       - é¿å…å­¦ä¹ çŽ‡è¿‡å°å¯¼è‡´çš„æ”¶æ•›åœæ»ž
    
    4. æ··åˆç²¾åº¦è®­ç»ƒï¼š
       - ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰æŠ€æœ¯
       - åœ¨ä¿æŒæ•°å€¼ç¨³å®šæ€§çš„åŒæ—¶æå‡è®­ç»ƒé€Ÿåº¦
       - ç‰¹åˆ«é€‚ç”¨äºŽå¤§æ¨¡åž‹è®­ç»ƒ
    
    ================================
    æ€§èƒ½ä¼˜åŒ–æŠ€æœ¯ï¼š
    ================================
    
    1. å†…å­˜ä¼˜åŒ–ï¼š
       - é€šé“ä¼˜å…ˆå†…å­˜æ ¼å¼ï¼ˆchannels_lastï¼‰æå‡GPUæ€§èƒ½
       - æ¢¯åº¦ç´¯ç§¯å‡å°‘æ˜¾å­˜å ç”¨
       - é”é¡µå†…å­˜åŠ é€ŸCPU-GPUæ•°æ®ä¼ è¾“
    
    2. è®¡ç®—ä¼˜åŒ–ï¼š
       - è‡ªåŠ¨æ··åˆç²¾åº¦å‡å°‘è®¡ç®—é‡
       - æ•°æ®å¹¶è¡ŒåŠ è½½æå‡I/Oæ•ˆçŽ‡
       - æ¢¯åº¦æ£€æŸ¥ç‚¹æœºåˆ¶å¤„ç†å¤§æ¨¡åž‹
    
    3. ç›‘æŽ§å’Œè°ƒè¯•ï¼š
       - å®žæ—¶æŸå¤±å’ŒæŒ‡æ ‡è®°å½•
       - æƒé‡å’Œæ¢¯åº¦åˆ†å¸ƒå¯è§†åŒ–
       - è®­ç»ƒè¿‡ç¨‹å›¾åƒå’Œé¢„æµ‹ç»“æžœå±•ç¤º
    
    ================================
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ================================
    
    ```python
    # åŸºç¡€è®­ç»ƒç¤ºä¾‹
    model = UNet(n_channels=3, n_classes=2)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    train_model(
        model=model,
        device=device,
        epochs=50,
        batch_size=8,
        learning_rate=1e-4,
        val_percent=0.2,
        amp=True
    )
    
    # é«˜æ€§èƒ½è®­ç»ƒç¤ºä¾‹
    train_model(
        model=model,
        device=device,
        epochs=100,
        batch_size=16,
        learning_rate=1e-4,
        img_scale=0.75,
        amp=True,
        weight_decay=1e-5
    )
    ```
    
    ================================
    æ³¨æ„äº‹é¡¹å’Œæœ€ä½³å®žè·µï¼š
    ================================
    
    1. æ•°æ®å‡†å¤‡ï¼š
       - ç¡®ä¿å›¾ç‰‡å’ŒæŽ©ç æ–‡ä»¶ä¸€ä¸€å¯¹åº”
       - æ£€æŸ¥å›¾ç‰‡æ ¼å¼å’Œé€šé“æ•°æ˜¯å¦ä¸€è‡´
       - éªŒè¯æŽ©ç æ ‡ç­¾å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
    
    2. ç¡¬ä»¶é…ç½®ï¼š
       - GPUè®­ç»ƒæ—¶ç¡®ä¿CUDAç‰ˆæœ¬å…¼å®¹
       - æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´batch_size
       - ä½¿ç”¨SSDå­˜å‚¨æå‡æ•°æ®åŠ è½½é€Ÿåº¦
    
    3. è¶…å‚æ•°è°ƒä¼˜ï¼š
       - å­¦ä¹ çŽ‡æ˜¯æœ€é‡è¦çš„è¶…å‚æ•°ï¼Œéœ€è¦ä»”ç»†è°ƒæ•´
       - æ‰¹æ¬¡å¤§å°å½±å“è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦
       - éªŒè¯é›†æ¯”ä¾‹å½±å“æ¨¡åž‹æ³›åŒ–èƒ½åŠ›è¯„ä¼°
    
    4. è®­ç»ƒç›‘æŽ§ï¼š
       - å®šæœŸæ£€æŸ¥è®­ç»ƒæŸå¤±å’ŒéªŒè¯æŒ‡æ ‡
       - å…³æ³¨è¿‡æ‹Ÿåˆå’Œæ¬ æ‹ŸåˆçŽ°è±¡
       - ä½¿ç”¨WandBç­‰å·¥å…·è¿›è¡Œå¯è§†åŒ–åˆ†æž
    
    5. å¼‚å¸¸å¤„ç†ï¼š
       - æ˜¾å­˜æº¢å‡ºæ—¶è‡ªåŠ¨å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
       - è®­ç»ƒä¸­æ–­æ—¶å¯ä»¥ä»Žæ£€æŸ¥ç‚¹æ¢å¤
       - ç½‘ç»œå¼‚å¸¸æ—¶è‡ªåŠ¨é‡è¯•æœºåˆ¶
    """
    # ================================
    # 1. æ•°æ®é›†åˆ›å»ºå’ŒåŠ è½½
    # ================================
    # ä¼˜å…ˆå°è¯•ä½¿ç”¨CarvanaDatasetï¼Œå¦‚æžœå¤±è´¥åˆ™å›žé€€åˆ°BasicDataset
    # CarvanaDatasetä¸“é—¨ä¸ºCarvanaæ±½è½¦åˆ†å‰²æ•°æ®é›†ä¼˜åŒ–ï¼ŒåŒ…å«ç‰¹å®šçš„æŽ©ç åŽç¼€å¤„ç†
    # BasicDatasetæ˜¯é€šç”¨æ•°æ®é›†ç±»ï¼Œæ”¯æŒå„ç§å›¾åƒåˆ†å‰²ä»»åŠ¡
    try:
        dataset = CarvanaDataset(Config.IMG_DIR, Config.MASK_DIR, img_scale)
        logging.info(f'æˆåŠŸåŠ è½½CarvanaDatasetï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬')
    except (AssertionError, RuntimeError, IndexError) as e:
        logging.warning(f'CarvanaDatasetåŠ è½½å¤±è´¥: {e}ï¼Œå›žé€€åˆ°BasicDataset')
        dataset = BasicDataset(Config.IMG_DIR, Config.MASK_DIR, img_scale)
        logging.info(f'æˆåŠŸåŠ è½½BasicDatasetï¼Œå…±{len(dataset)}ä¸ªæ ·æœ¬')

    # ================================
    # 2. è®­ç»ƒé›†å’ŒéªŒè¯é›†åˆ’åˆ†
    # ================================
    # æŒ‰ç…§æŒ‡å®šæ¯”ä¾‹å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
    # ä½¿ç”¨å›ºå®šéšæœºç§å­ç¡®ä¿æ¯æ¬¡è¿è¡Œçš„æ•°æ®åˆ’åˆ†ç»“æžœä¸€è‡´
    n_val = int(len(dataset) * val_percent)  # è®¡ç®—éªŒè¯é›†æ ·æœ¬æ•°é‡
    n_train = len(dataset) - n_val           # è®¡ç®—è®­ç»ƒé›†æ ·æœ¬æ•°é‡
    
    # ä½¿ç”¨random_splitè¿›è¡Œæ•°æ®åˆ’åˆ†ï¼Œç¡®ä¿å¯é‡å¤æ€§
    # generatorå‚æ•°ä½¿ç”¨å›ºå®šç§å­ï¼Œä¿è¯å®žéªŒçš„å¯é‡å¤æ€§
    train_set, val_set = random_split(
        dataset, 
        [n_train, n_val], 
        generator=torch.Generator().manual_seed(0)
    )
    
    logging.info(f'æ•°æ®é›†åˆ’åˆ†å®Œæˆï¼šè®­ç»ƒé›†{n_train}ä¸ªæ ·æœ¬ï¼ŒéªŒè¯é›†{n_val}ä¸ªæ ·æœ¬')

    # ================================
    # 3. æ•°æ®åŠ è½½å™¨é…ç½®å’Œåˆ›å»º
    # ================================
    # é…ç½®é«˜æ•ˆçš„æ•°æ®åŠ è½½å™¨å‚æ•°ï¼Œå¹³è¡¡å†…å­˜ä½¿ç”¨å’ŒåŠ è½½é€Ÿåº¦
    loader_args = dict(
        batch_size=batch_size,                    # æ‰¹æ¬¡å¤§å°ï¼Œæ ¹æ®æ˜¾å­˜è°ƒæ•´
        num_workers=Config.NUM_WORKERS,          # æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼Œé¿å…CPUè¿‡è½½
        pin_memory=Config.PIN_MEMORY,            # GPUè®­ç»ƒæ—¶å¯ç”¨é”é¡µå†…å­˜ï¼ŒåŠ é€Ÿæ•°æ®ä¼ è¾“
        prefetch_factor=Config.PREFETCH_FACTOR,  # é¢„åŠ è½½æ‰¹æ¬¡æ•°ï¼Œå¹³è¡¡å†…å­˜å’Œé€Ÿåº¦
        persistent_workers=True                   # ä¿æŒworkerè¿›ç¨‹å­˜æ´»ï¼Œå‡å°‘é‡å¯å¼€é”€
    )
    
    # åˆ›å»ºå›ºå®šéšæœºç§å­çš„ç”Ÿæˆå™¨ï¼Œç¡®ä¿è®­ç»ƒè¿‡ç¨‹çš„å¯é‡å¤æ€§
    g = torch.Generator()
    g.manual_seed(Config.RANDOM_SEED)
    
    # åˆ›å»ºè®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_set,
        shuffle=True,          # è®­ç»ƒæ—¶æ‰“ä¹±æ•°æ®é¡ºåºï¼Œæé«˜æ¨¡åž‹æ³›åŒ–èƒ½åŠ›
        drop_last=True,        # ä¸¢å¼ƒä¸å®Œæ•´çš„æœ€åŽä¸€æ‰¹ï¼Œé¿å…æ‰¹æ¬¡å¤§å°ä¸ä¸€è‡´å½±å“è®­ç»ƒç¨³å®šæ€§
        generator=g,           # ä½¿ç”¨å›ºå®šç§å­çš„ç”Ÿæˆå™¨
        **loader_args
    )
    
    # åˆ›å»ºéªŒè¯æ•°æ®åŠ è½½å™¨
    val_loader = DataLoader(
        val_set,
        shuffle=False,         # éªŒè¯æ—¶ä¸æ‰“ä¹±æ•°æ®ï¼Œç¡®ä¿è¯„ä¼°ç»“æžœçš„ä¸€è‡´æ€§
        drop_last=True,        # åŒæ ·ä¸¢å¼ƒä¸å®Œæ•´çš„æ‰¹æ¬¡
        **loader_args
    )

    logging.info(f'æ•°æ®åŠ è½½å™¨åˆ›å»ºå®Œæˆï¼šè®­ç»ƒæ‰¹æ¬¡{len(train_loader)}ä¸ªï¼ŒéªŒè¯æ‰¹æ¬¡{len(val_loader)}ä¸ª')

    # ================================
    # 4. å®žéªŒè·Ÿè¸ªå’Œæ—¥å¿—åˆå§‹åŒ–
    # ================================
    # åˆå§‹åŒ–WandBå®žéªŒè·Ÿè¸ªï¼Œç”¨äºŽè®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–å’Œè¶…å‚æ•°ç®¡ç†
    # resume='allow': å…è®¸æ¢å¤ä¸­æ–­çš„å®žéªŒ
    # anonymous='must': å…è®¸åŒ¿åä½¿ç”¨WandB
    experiment = wandb.init(
        project='U-Net', 
        resume='allow', 
        anonymous='must',
        name=f'unet_training_{epochs}epochs_bs{batch_size}'
    )
    
    # è®°å½•è®­ç»ƒé…ç½®å‚æ•°åˆ°WandB
    experiment.config.update(
        dict(
            epochs=epochs, 
            batch_size=batch_size, 
            learning_rate=learning_rate,
            val_percent=val_percent, 
            save_checkpoint=save_checkpoint, 
            img_scale=img_scale, 
            amp=amp,
            weight_decay=weight_decay,
            gradient_clipping=gradient_clipping
        )
    )

    # æ‰“å°è¯¦ç»†çš„è®­ç»ƒå‚æ•°ä¿¡æ¯
    logging.info(f'''
    ================================
    è®­ç»ƒé…ç½®ä¿¡æ¯
    ================================
    è®­ç»ƒè½®æ•°:           {epochs}
    æ‰¹æ¬¡å¤§å°:           {batch_size}
    å­¦ä¹ çŽ‡:             {learning_rate}
    è®­ç»ƒé›†å¤§å°:         {n_train}
    éªŒè¯é›†å¤§å°:         {n_val}
    éªŒè¯é›†æ¯”ä¾‹:         {val_percent:.1%}
    æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹:     {save_checkpoint}
    è®­ç»ƒè®¾å¤‡:           {device.type}
    å›¾ç‰‡ç¼©æ”¾æ¯”ä¾‹:       {img_scale}
    æ··åˆç²¾åº¦è®­ç»ƒ:       {amp}
    æƒé‡è¡°å‡:           {weight_decay}
    æ¢¯åº¦è£å‰ªé˜ˆå€¼:       {gradient_clipping}
    ================================
    ''')

    # ================================
    # 5. è®­ç»ƒç»„ä»¶åˆå§‹åŒ–
    # ================================
    
    # 5.1 ä¼˜åŒ–å™¨é…ç½®
    # ä½¿ç”¨AdamWä¼˜åŒ–å™¨ï¼Œç»“åˆäº†Adamçš„è‡ªé€‚åº”å­¦ä¹ çŽ‡å’Œæƒé‡è¡°å‡çš„ä¼˜åŠ¿
    # AdamWç›¸æ¯”Adamæœ‰æ›´å¥½çš„æ³›åŒ–æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¸­
    optimizer = optim.AdamW(
        model.parameters(),
        lr=learning_rate,           # åˆå§‹å­¦ä¹ çŽ‡
        weight_decay=weight_decay,  # L2æ­£åˆ™åŒ–ç³»æ•°ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ
        amsgrad=True               # å¯ç”¨AMSGradå˜ä½“ï¼Œæä¾›æ›´ç¨³å®šçš„è®­ç»ƒè¿‡ç¨‹
    )
    
    # 5.2 å­¦ä¹ çŽ‡è°ƒåº¦å™¨é…ç½®
    # ä½¿ç”¨ä½™å¼¦é€€ç«é‡å¯ç­–ç•¥ï¼Œåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ çŽ‡
    # è¿™ç§ç­–ç•¥å¯ä»¥åœ¨è®­ç»ƒåŽæœŸæä¾›æ›´å¥½çš„æ”¶æ•›æ€§èƒ½
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer,
        T_0=epochs // 3,                    # ç¬¬ä¸€æ¬¡é‡å¯çš„å‘¨æœŸé•¿åº¦
        T_mult=2,                          # æ¯æ¬¡é‡å¯åŽå‘¨æœŸé•¿åº¦ç¿»å€
        eta_min=learning_rate * 1e-3       # æœ€å°å­¦ä¹ çŽ‡ï¼Œé˜²æ­¢å­¦ä¹ çŽ‡è¿‡å°
    )
    
    # 5.3 æ··åˆç²¾åº¦è®­ç»ƒç¼©æ”¾å™¨
    # ç”¨äºŽæ··åˆç²¾åº¦è®­ç»ƒçš„æ¢¯åº¦ç¼©æ”¾ï¼Œç¡®ä¿è®­ç»ƒç¨³å®šæ€§
    # å½“ä½¿ç”¨FP16ç²¾åº¦æ—¶ï¼Œæ¢¯åº¦å¯èƒ½è¿‡å°ï¼Œéœ€è¦æ”¾å¤§åŽæ›´æ–°å‚æ•°
    try:
        # ä½¿ç”¨æ–°çš„API (PyTorch 2.0+)
        grad_scaler = torch.amp.GradScaler('cuda', enabled=amp)
    except AttributeError:
        # å‘åŽå…¼å®¹æ—§ç‰ˆæœ¬PyTorch
        grad_scaler = torch.cuda.amp.GradScaler(enabled=amp)
    
    # 5.4 æŸå¤±å‡½æ•°é…ç½®
    # æ ¹æ®ä»»åŠ¡ç±»åž‹é€‰æ‹©åˆé€‚çš„æŸå¤±å‡½æ•°
    # å¤šåˆ†ç±»ä»»åŠ¡ä½¿ç”¨äº¤å‰ç†µæŸå¤±ï¼ŒäºŒåˆ†ç±»ä»»åŠ¡ä½¿ç”¨BCEæŸå¤±
    if model.n_classes > 1:
        criterion = nn.CrossEntropyLoss()  # å¤šåˆ†ç±»äº¤å‰ç†µæŸå¤±
        logging.info('ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆå¤šåˆ†ç±»ä»»åŠ¡ï¼‰')
    else:
        criterion = nn.BCEWithLogitsLoss()  # äºŒåˆ†ç±»BCEæŸå¤±
        logging.info('ä½¿ç”¨BCEæŸå¤±å‡½æ•°ï¼ˆäºŒåˆ†ç±»ä»»åŠ¡ï¼‰')
    
    # 5.5 è®­ç»ƒçŠ¶æ€å˜é‡
    global_step = 0  # å…¨å±€è®­ç»ƒæ­¥æ•°è®¡æ•°å™¨ï¼Œç”¨äºŽWandBæ—¥å¿—è®°å½•
    
    logging.info('è®­ç»ƒç»„ä»¶åˆå§‹åŒ–å®Œæˆ')

    # ================================
    # 6. ä¸»è®­ç»ƒå¾ªçŽ¯
    # ================================
    # å¼€å§‹é€è½®è®­ç»ƒï¼Œæ¯è½®éåŽ†æ•´ä¸ªè®­ç»ƒé›†ä¸€æ¬¡
    for epoch in range(1, epochs + 1):
        # å°†æ¨¡åž‹è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
        # è¿™ä¼šå¯ç”¨dropoutã€batch normalizationçš„è®­ç»ƒæ¨¡å¼ç­‰è®­ç»ƒæ—¶ç‰¹æœ‰çš„è¡Œä¸º
        model.train()
        epoch_loss = 0  # å½“å‰è½®æ¬¡çš„ç´¯ç§¯æŸå¤±
        
        # åˆ›å»ºè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºå½“å‰è½®æ¬¡çš„è®­ç»ƒè¿›åº¦
        with tqdm(total=n_train, desc=f'Epoch {epoch}/{epochs}', unit='img') as pbar:
            # éåŽ†è®­ç»ƒé›†ä¸­çš„æ‰€æœ‰æ‰¹æ¬¡
            for batch in train_loader:
                # ä»Žæ‰¹æ¬¡ä¸­æå–å›¾åƒå’ŒçœŸå®žæŽ©ç 
                images, true_masks = batch['image'], batch['mask']

                # éªŒè¯å›¾åƒé€šé“æ•°æ˜¯å¦ä¸Žæ¨¡åž‹æœŸæœ›ä¸€è‡´
                # è¿™æ˜¯é‡è¦çš„å®‰å…¨æ£€æŸ¥ï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…é”™è¯¯
                assert images.shape[1] == model.n_channels, \
                    f'æ¨¡åž‹å®šä¹‰ä¸º{model.n_channels}è¾“å…¥é€šé“ï¼Œä½†å›¾ç‰‡å®žé™…ä¸º{images.shape[1]}é€šé“ï¼Œè¯·æ£€æŸ¥å›¾ç‰‡åŠ è½½æ˜¯å¦æ­£ç¡®ã€‚'

                # å°†æ•°æ®ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡å¹¶è½¬æ¢ä¸ºåˆé€‚çš„ç±»åž‹
                # images: è½¬æ¢ä¸ºfloat32ç±»åž‹ï¼Œä½¿ç”¨channels_lastå†…å­˜æ ¼å¼ä¼˜åŒ–GPUæ€§èƒ½
                # true_masks: è½¬æ¢ä¸ºlongç±»åž‹ï¼Œå› ä¸ºæŽ©ç æ ‡ç­¾é€šå¸¸æ˜¯æ•´æ•°ç´¢å¼•
                images = images.to(device=device, dtype=torch.float32, memory_format=torch.channels_last)
                true_masks = true_masks.to(device=device, dtype=torch.long)

                # ================================
                # 6.1 å‰å‘ä¼ æ’­ä¸ŽæŸå¤±è®¡ç®—
                # ================================
                # ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œæå‡è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘æ˜¾å­˜å ç”¨
                # å¯¹äºŽMPSè®¾å¤‡éœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œå› ä¸ºApple Siliconå¯¹autocastæ”¯æŒæœ‰é™
                with torch.autocast(device.type if device.type != 'mps' else 'cpu', enabled=amp):
                    # æ¨¡åž‹å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°é¢„æµ‹ç»“æžœ
                    # masks_predå½¢çŠ¶ï¼š(batch_size, n_classes, height, width)
                    masks_pred = model(images)
                    
                    # æ ¹æ®ä»»åŠ¡ç±»åž‹è®¡ç®—ä¸åŒçš„æŸå¤±å‡½æ•°ç»„åˆ
                    if model.n_classes == 1:
                        # ================================
                        # äºŒåˆ†ç±»ä»»åŠ¡æŸå¤±è®¡ç®—
                        # ================================
                        # ä½¿ç”¨BCEæŸå¤± + DiceæŸå¤±çš„ç»„åˆ
                        # BCEæŸå¤±ï¼šäºŒå…ƒäº¤å‰ç†µï¼Œå¤„ç†å‰æ™¯/èƒŒæ™¯åˆ†å‰²
                        bce_loss = criterion(masks_pred.squeeze(1), true_masks.float())
                        
                        # DiceæŸå¤±ï¼šä¸“é—¨ä¼˜åŒ–åˆ†å‰²é‡å åº¦ï¼Œæ”¹å–„è¾¹ç•Œåˆ†å‰²æ•ˆæžœ
                        # ä½¿ç”¨sigmoidæ¿€æ´»å‡½æ•°å°†logitsè½¬æ¢ä¸ºæ¦‚çŽ‡
                        dice_loss_value = dice_loss(
                            F.sigmoid(masks_pred.squeeze(1)), 
                            true_masks.float(), 
                            multiclass=False
                        )
                        
                        # æ€»æŸå¤± = BCEæŸå¤± + DiceæŸå¤±
                        # è¿™ç§ç»„åˆåœ¨åŒ»å­¦å›¾åƒåˆ†å‰²ä¸­è¡¨çŽ°ä¼˜å¼‚
                        loss = bce_loss + dice_loss_value
                        
                    else:
                        # ================================
                        # å¤šåˆ†ç±»ä»»åŠ¡æŸå¤±è®¡ç®—
                        # ================================
                        # ä½¿ç”¨äº¤å‰ç†µæŸå¤± + DiceæŸå¤±çš„ç»„åˆ
                        # äº¤å‰ç†µæŸå¤±ï¼šå¤„ç†å¤šä¸ªç±»åˆ«çš„å‰æ™¯åˆ†å‰²
                        ce_loss = criterion(masks_pred, true_masks)
                        
                        # å°†é¢„æµ‹ç»“æžœè½¬æ¢ä¸ºæ¦‚çŽ‡åˆ†å¸ƒï¼ˆsoftmaxï¼‰
                        pred_probs = F.softmax(masks_pred, dim=1).float()
                        
                        # å°†çœŸå®žæ ‡ç­¾è½¬æ¢ä¸ºone-hotç¼–ç æ ¼å¼
                        # one_hot: (batch_size, height, width) -> (batch_size, height, width, n_classes)
                        # permute: è°ƒæ•´ç»´åº¦é¡ºåºä¸º(batch_size, n_classes, height, width)
                        true_one_hot = F.one_hot(true_masks, model.n_classes).permute(0, 3, 1, 2).float()
                        
                        # è®¡ç®—å¤šåˆ†ç±»DiceæŸå¤±
                        dice_loss_value = dice_loss(pred_probs, true_one_hot, multiclass=True)
                        
                        # æ€»æŸå¤± = äº¤å‰ç†µæŸå¤± + DiceæŸå¤±
                        loss = ce_loss + dice_loss_value

                # ================================
                # 6.2 åå‘ä¼ æ’­ä¸Žå‚æ•°ä¼˜åŒ–
                # ================================
                # æ¸…ç©ºæ¢¯åº¦ç¼“å­˜ï¼Œset_to_none=Trueå¯ä»¥èŠ‚çœå†…å­˜
                optimizer.zero_grad(set_to_none=True)
                
                # æ¢¯åº¦ç¼©æ”¾åå‘ä¼ æ’­
                # åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼ŒæŸå¤±éœ€è¦å…ˆæ”¾å¤§å†åå‘ä¼ æ’­
                # è¿™æ ·å¯ä»¥é¿å…æ¢¯åº¦ä¸‹æº¢é—®é¢˜ï¼Œä¿æŒè®­ç»ƒç¨³å®šæ€§
                grad_scaler.scale(loss).backward()
                
                # å–æ¶ˆæ¢¯åº¦ç¼©æ”¾ï¼Œå‡†å¤‡è¿›è¡Œæ¢¯åº¦è£å‰ªå’Œå‚æ•°æ›´æ–°
                grad_scaler.unscale_(optimizer)
                
                # æ¢¯åº¦è£å‰ªï¼šé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                # å½“æ¢¯åº¦çš„L2èŒƒæ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾æ‰€æœ‰æ¢¯åº¦
                # è¿™æ˜¯RNNå’Œæ·±åº¦ç½‘ç»œè®­ç»ƒä¸­çš„é‡è¦æŠ€æœ¯
                torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clipping)
                
                # æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤ï¼Œæ›´æ–°æ¨¡åž‹å‚æ•°
                # åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œéœ€è¦å…ˆç¼©æ”¾æ¢¯åº¦å†æ›´æ–°
                grad_scaler.step(optimizer)
                
                # æ›´æ–°æ¢¯åº¦ç¼©æ”¾å™¨çš„å†…éƒ¨çŠ¶æ€
                # æ ¹æ®æ˜¯å¦å‘ç”Ÿæ¢¯åº¦æº¢å‡ºï¼ŒåŠ¨æ€è°ƒæ•´ç¼©æ”¾å› å­
                grad_scaler.update()

                # ================================
                # 6.3 è®­ç»ƒçŠ¶æ€æ›´æ–°å’Œæ—¥å¿—è®°å½•
                # ================================
                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤º
                pbar.update(images.shape[0])
                global_step += 1
                epoch_loss += loss.item()  # ç´¯åŠ å½“å‰è½®æ¬¡çš„æŸå¤±
                
                # è®°å½•è®­ç»ƒæŒ‡æ ‡åˆ°WandB
                # å®žæ—¶ç›‘æŽ§è®­ç»ƒè¿‡ç¨‹ï¼Œä¾¿äºŽè°ƒè¯•å’Œä¼˜åŒ–
                experiment.log({
                    'train loss': loss.item(),      # å½“å‰æ‰¹æ¬¡çš„æŸå¤±å€¼
                    'step': global_step,            # å…¨å±€è®­ç»ƒæ­¥æ•°
                    'epoch': epoch,                 # å½“å‰è½®æ¬¡
                    'learning rate': optimizer.param_groups[0]['lr']  # å½“å‰å­¦ä¹ çŽ‡
                })
                
                # æ›´æ–°è¿›åº¦æ¡åŽç¼€ï¼Œæ˜¾ç¤ºå½“å‰æ‰¹æ¬¡çš„æŸå¤±
                pbar.set_postfix(**{'loss (batch)': loss.item()})

                # ================================
                # 6.4 å®šæœŸéªŒè¯å’Œå¯è§†åŒ–è®°å½•
                # ================================
                # è®¡ç®—éªŒè¯é—´éš”æ­¥æ•°ï¼Œæ¯è½®è®­ç»ƒè¿›è¡Œ5æ¬¡éªŒè¯
                # è¿™æ ·å¯ä»¥æ›´é¢‘ç¹åœ°ç›‘æŽ§æ¨¡åž‹æ€§èƒ½ï¼ŒåŠæ—¶å‘çŽ°è¿‡æ‹Ÿåˆç­‰é—®é¢˜
                division_step = (n_train // (5 * batch_size))
                if division_step > 0:
                    if global_step % division_step == 0:
                        # æ”¶é›†æ¨¡åž‹æƒé‡å’Œæ¢¯åº¦çš„åˆ†å¸ƒä¿¡æ¯
                        # ç”¨äºŽç›‘æŽ§è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ¢¯åº¦æµåŠ¨æƒ…å†µ
                        histograms = {}
                        for tag, value in model.named_parameters():
                            tag = tag.replace('/', '.')  # å°†è·¯å¾„åˆ†éš”ç¬¦æ›¿æ¢ä¸ºç‚¹å·ï¼Œä¾¿äºŽWandBæ˜¾ç¤º
                            
                            # è®°å½•æƒé‡åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆæŽ’é™¤æ— ç©·å¤§å’ŒNaNå€¼ï¼‰
                            try:
                                if value.grad is not None and not (torch.isinf(value) | torch.isnan(value)).any():
                                    # ç¡®ä¿å¼ é‡æ˜¯è¿žç»­çš„ä¸”éžç¨€ç–çš„
                                    weight_data = value.data.cpu().contiguous()
                                    if not weight_data.is_sparse:
                                        histograms['Weights/' + tag] = wandb.Histogram(weight_data)
                            except Exception:
                                # å¦‚æžœè®°å½•æƒé‡å¤±è´¥ï¼Œè·³è¿‡ä½†ä¸å½±å“è®­ç»ƒ
                                pass
                            
                            # è®°å½•æ¢¯åº¦åˆ†å¸ƒç›´æ–¹å›¾ï¼ˆæŽ’é™¤æ— ç©·å¤§å’ŒNaNå€¼ï¼‰
                            try:
                                if value.grad is not None and not (torch.isinf(value.grad) | torch.isnan(value.grad)).any():
                                    # ç¡®ä¿æ¢¯åº¦å¼ é‡æ˜¯è¿žç»­çš„ä¸”éžç¨€ç–çš„
                                    grad_data = value.grad.data.cpu().contiguous()
                                    if not grad_data.is_sparse:
                                        histograms['Gradients/' + tag] = wandb.Histogram(grad_data)
                            except Exception:
                                # å¦‚æžœè®°å½•æ¢¯åº¦å¤±è´¥ï¼Œè·³è¿‡ä½†ä¸å½±å“è®­ç»ƒ
                                pass

                        # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡åž‹æ€§èƒ½
                        # è¿™ä¼šåˆ‡æ¢åˆ°è¯„ä¼°æ¨¡å¼ï¼Œè®¡ç®—Diceç³»æ•°ç­‰æŒ‡æ ‡
                        val_score = evaluate(model, val_loader, device, amp)
                        
                        # æ ¹æ®éªŒè¯åˆ†æ•°æ›´æ–°å­¦ä¹ çŽ‡è°ƒåº¦å™¨
                        # ä½™å¼¦é€€ç«è°ƒåº¦å™¨ä¼šæ ¹æ®éªŒè¯æ€§èƒ½è°ƒæ•´å­¦ä¹ çŽ‡
                        scheduler.step(val_score)

                        # è®°å½•éªŒè¯ç»“æžœ
                        logging.info(f'éªŒè¯é›†Diceåˆ†æ•°: {val_score:.4f}')
                        
                        # è®°å½•è¯¦ç»†çš„éªŒè¯ä¿¡æ¯åˆ°WandB
                        try:
                            # å¤„ç†é¢„æµ‹æŽ©ç çš„ç»´åº¦é—®é¢˜ï¼Œç¡®ä¿ä¸ŽWandBå…¼å®¹
                            if model.n_classes == 1:
                                # äºŒåˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨sigmoid + é˜ˆå€¼å¤„ç†
                                pred_mask = (F.sigmoid(masks_pred[0, 0]) > 0.5).float().cpu()
                            else:
                                # å¤šåˆ†ç±»ä»»åŠ¡ï¼šä½¿ç”¨argmaxå¤„ç†
                                pred_mask = masks_pred.argmax(dim=1)[0].float().cpu()
                            
                            # ç¡®ä¿æŽ©ç å¼ é‡æ˜¯æ­£ç¡®çš„2Dæ ¼å¼ç”¨äºŽWandB
                            if pred_mask.dim() > 2:
                                pred_mask = pred_mask.squeeze()
                            if pred_mask.dim() < 2:
                                pred_mask = pred_mask.unsqueeze(0)
                                
                            # ç¡®ä¿çœŸå®žæŽ©ç ä¹Ÿæ˜¯2Dæ ¼å¼
                            true_mask = true_masks[0].float().cpu()
                            if true_mask.dim() > 2:
                                true_mask = true_mask.squeeze()
                            if true_mask.dim() < 2:
                                true_mask = true_mask.unsqueeze(0)
                            
                            experiment.log({
                                'learning rate': optimizer.param_groups[0]['lr'],  # å½“å‰å­¦ä¹ çŽ‡
                                'validation Dice': val_score,                       # éªŒè¯é›†Diceåˆ†æ•°
                                'images': wandb.Image(images[0].cpu()),            # è¾“å…¥å›¾åƒ
                                'masks': {                                         # æŽ©ç å¯¹æ¯”
                                    'true': wandb.Image(true_mask),                # çœŸå®žæŽ©ç 
                                    'pred': wandb.Image(pred_mask),                # é¢„æµ‹æŽ©ç 
                                },
                                'step': global_step,                               # å…¨å±€æ­¥æ•°
                                'epoch': epoch,                                    # å½“å‰è½®æ¬¡
                                **histograms                                       # æƒé‡å’Œæ¢¯åº¦åˆ†å¸ƒ
                            })
                        except Exception as e:
                            # å¦‚æžœè®°å½•å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒè€Œä¸ä¸­æ–­
                            logging.warning(f'WandBè®°å½•å¤±è´¥: {e}')
                            pass

        # ================================
        # 6.5 æ¨¡åž‹æ£€æŸ¥ç‚¹ä¿å­˜
        # ================================
        # æ¯è½®è®­ç»ƒç»“æŸåŽä¿å­˜æ¨¡åž‹çŠ¶æ€ï¼Œæ”¯æŒæ–­ç‚¹ç»­è®­å’Œæ¨¡åž‹æ¢å¤
        if save_checkpoint:
            # ç¡®ä¿æ£€æŸ¥ç‚¹ç›®å½•å­˜åœ¨
            Path(Config.CHECKPOINT_DIR).mkdir(parents=True, exist_ok=True)
            
            # æž„å»ºæ£€æŸ¥ç‚¹æ•°æ®å­—å…¸
            # åŒ…å«è®­ç»ƒæ¢å¤æ‰€éœ€çš„æ‰€æœ‰çŠ¶æ€ä¿¡æ¯
            checkpoint = {
                'epoch': epoch,                                    # å½“å‰è®­ç»ƒè½®æ¬¡
                'model_state_dict': model.state_dict(),           # æ¨¡åž‹å‚æ•°çŠ¶æ€
                'optimizer_state_dict': optimizer.state_dict(),   # ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆåŒ…æ‹¬åŠ¨é‡ç­‰ï¼‰
                'scheduler_state_dict': scheduler.state_dict(),   # å­¦ä¹ çŽ‡è°ƒåº¦å™¨çŠ¶æ€
                'loss': epoch_loss / len(train_loader),           # å¹³å‡æŸå¤±
                'mask_values': dataset.mask_values                # æ•°æ®é›†æŽ©ç å€¼ï¼ˆç”¨äºŽæŽ¨ç†æ—¶çš„æ ‡ç­¾æ˜ å°„ï¼‰
            }
            
            # ä¿å­˜æ£€æŸ¥ç‚¹æ–‡ä»¶
            checkpoint_path = Config.CHECKPOINT_DIR / f'checkpoint_epoch{epoch}.pth'
            torch.save(checkpoint, str(checkpoint_path))
            logging.info(f'æ£€æŸ¥ç‚¹ {epoch} å·²ä¿å­˜è‡³ {checkpoint_path}!')
            
            # è®°å½•æ£€æŸ¥ç‚¹ä¿¡æ¯åˆ°WandB
            experiment.log({
                'epoch': epoch,
                'avg_train_loss': epoch_loss / len(train_loader),
                'checkpoint_saved': True
            })



def get_args():
    """
    å‘½ä»¤è¡Œå‚æ•°è§£æžå‡½æ•°ï¼Œå¤„ç†è®­ç»ƒè„šæœ¬çš„å‘½ä»¤è¡Œè¾“å…¥å‚æ•°
    
    è¯¥å‡½æ•°ä½¿ç”¨argparseæ¨¡å—è§£æžå‘½ä»¤è¡Œå‚æ•°ï¼Œæä¾›äº†çµæ´»çš„è®­ç»ƒé…ç½®é€‰é¡¹ã€‚
    æ‰€æœ‰å‚æ•°éƒ½æœ‰åˆç†çš„é»˜è®¤å€¼ï¼Œæ”¯æŒå¿«é€Ÿå¼€å§‹è®­ç»ƒæˆ–ç²¾ç»†è°ƒä¼˜ã€‚
    
    ================================
    æ”¯æŒçš„å‚æ•°è¯¦è§£ï¼š
    ================================
    
    åŸºç¡€è®­ç»ƒå‚æ•°ï¼š
    --epochs, -e (int): è®­ç»ƒè½®æ•°
        - é»˜è®¤å€¼: 5
        - å»ºè®®å€¼: å°æ•°æ®é›†50-100è½®ï¼Œå¤§æ•°æ®é›†20-50è½®
        - å½±å“: è¿‡å°‘å¯èƒ½å¯¼è‡´æ¬ æ‹Ÿåˆï¼Œè¿‡å¤šå¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ
    
    --batch-size, -b (int): æ‰¹æ¬¡å¤§å°
        - é»˜è®¤å€¼: 1
        - å»ºè®®å€¼: æ ¹æ®æ˜¾å­˜è°ƒæ•´ï¼Œ8GBæ˜¾å­˜å¯ç”¨4-8ï¼Œ16GBå¯ç”¨8-16
        - å½±å“: å½±å“è®­ç»ƒç¨³å®šæ€§å’Œæ˜¾å­˜å ç”¨
    
    --learning-rate, -l (float): å­¦ä¹ çŽ‡
        - é»˜è®¤å€¼: 1e-5
        - å»ºè®®èŒƒå›´: 1e-4 åˆ° 1e-6
        - å½±å“: æŽ§åˆ¶æ¨¡åž‹å‚æ•°æ›´æ–°çš„æ­¥é•¿
    
    æ•°æ®å’Œæ¨¡åž‹å‚æ•°ï¼š
    --load, -f (str): é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„
        - é»˜è®¤å€¼: Falseï¼ˆä¸ä½¿ç”¨é¢„è®­ç»ƒæ¨¡åž‹ï¼‰
        - ç”¨é€”: ä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒæˆ–ä½¿ç”¨é¢„è®­ç»ƒæƒé‡
        - æ ¼å¼: .pthæ–‡ä»¶è·¯å¾„
    
    --scale, -s (float): å›¾åƒç¼©æ”¾æ¯”ä¾‹
        - é»˜è®¤å€¼: 0.5
        - èŒƒå›´: (0, 1]
        - å½±å“: æŽ§åˆ¶è¾“å…¥å›¾åƒå°ºå¯¸ï¼Œå¹³è¡¡æ˜¾å­˜å ç”¨å’Œåˆ†å‰²ç²¾åº¦
    
    --validation, -v (float): éªŒè¯é›†æ¯”ä¾‹
        - é»˜è®¤å€¼: 10.0ï¼ˆè¡¨ç¤º10%ï¼‰
        - èŒƒå›´: 0-100
        - å»ºè®®å€¼: å°æ•°æ®é›†20-30%ï¼Œå¤§æ•°æ®é›†10-20%
    
    --classes, -c (int): åˆ†å‰²ç±»åˆ«æ•°
        - é»˜è®¤å€¼: 2
        - å«ä¹‰: åŒ…æ‹¬èƒŒæ™¯ç±»åœ¨å†…çš„æ€»ç±»åˆ«æ•°
        - å½±å“: å†³å®šæ¨¡åž‹è¾“å‡ºé€šé“æ•°å’ŒæŸå¤±å‡½æ•°é€‰æ‹©
    
    ä¼˜åŒ–å’Œæ€§èƒ½å‚æ•°ï¼š
    --amp (flag): å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
        - é»˜è®¤å€¼: False
        - æ•ˆæžœ: å‡å°‘50%æ˜¾å­˜å ç”¨ï¼Œæå‡è®­ç»ƒé€Ÿåº¦
        - é€‚ç”¨: æ˜¾å­˜ä¸è¶³æˆ–éœ€è¦åŠ é€Ÿè®­ç»ƒæ—¶
    
    --bilinear (flag): ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
        - é»˜è®¤å€¼: Falseï¼ˆä½¿ç”¨è½¬ç½®å·ç§¯ï¼‰
        - æ•ˆæžœ: å‡å°‘å‚æ•°é‡ï¼Œä½†å¯èƒ½å½±å“åˆ†å‰²ç²¾åº¦
        - é€‚ç”¨: æ¨¡åž‹åŽ‹ç¼©æˆ–æ˜¾å­˜æžåº¦ä¸è¶³æ—¶
    
    ================================
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    ================================
    
    # åŸºç¡€è®­ç»ƒ
    python train.py --epochs 50 --batch-size 8 --learning-rate 1e-4
    
    # é«˜æ€§èƒ½è®­ç»ƒï¼ˆæ··åˆç²¾åº¦+å¤§æ‰¹æ¬¡ï¼‰
    python train.py --epochs 100 --batch-size 16 --amp --scale 0.75
    
    # ä»Žæ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ
    python train.py --load checkpoints/checkpoint_epoch10.pth --epochs 50
    
    # å¤šåˆ†ç±»åˆ†å‰²ä»»åŠ¡
    python train.py --classes 5 --epochs 80 --batch-size 4
    
    # æ˜¾å­˜ä¼˜åŒ–è®­ç»ƒ
    python train.py --amp --bilinear --batch-size 2 --scale 0.25
    
    ================================
    è¿”å›žå€¼å’Œå¼‚å¸¸ï¼š
    ================================
    
    Returns:
        argparse.Namespace: è§£æžåŽçš„å‚æ•°å¯¹è±¡ï¼ŒåŒ…å«æ‰€æœ‰è®­ç»ƒé…ç½®
        
    Raises:
        SystemExit: å½“å‚æ•°è§£æžå¤±è´¥æˆ–ç”¨æˆ·è¯·æ±‚å¸®åŠ©æ—¶é€€å‡ºç¨‹åº
        
    ================================
    æ³¨æ„äº‹é¡¹ï¼š
    ================================
    
    1. å‚æ•°éªŒè¯ï¼š
       - æ‰€æœ‰æ•°å€¼å‚æ•°éƒ½æœ‰èŒƒå›´æ£€æŸ¥
       - æ–‡ä»¶è·¯å¾„ä¼šè‡ªåŠ¨éªŒè¯å­˜åœ¨æ€§
       - ä¸åˆç†çš„å‚æ•°ç»„åˆä¼šç»™å‡ºè­¦å‘Š
    
    2. é»˜è®¤å€¼è®¾è®¡ï¼š
       - æ‰€æœ‰å‚æ•°éƒ½æœ‰ç»è¿‡æµ‹è¯•çš„é»˜è®¤å€¼
       - é»˜è®¤é…ç½®é€‚åˆå¤§å¤šæ•°åœºæ™¯
       - å¯ä»¥æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´
    
    3. å…¼å®¹æ€§ï¼š
       - æ”¯æŒçŸ­å‚æ•°åå’Œé•¿å‚æ•°å
       - å¸ƒå°”æ ‡å¿—ä¸éœ€è¦æŒ‡å®šå€¼
       - æ•°å€¼å‚æ•°æ”¯æŒç§‘å­¦è®¡æ•°æ³•
    
    4. é”™è¯¯å¤„ç†ï¼š
       - æ— æ•ˆå‚æ•°ä¼šç»™å‡ºæ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
       - æä¾›ä½¿ç”¨å¸®åŠ©å’Œç¤ºä¾‹
       - è‡ªåŠ¨æ£€æµ‹å¸¸è§é…ç½®é”™è¯¯
    """
    parser = argparse.ArgumentParser(description='è®­ç»ƒUNetç”¨äºŽå›¾åƒä¸ŽæŽ©ç åˆ†å‰²')
    parser.add_argument('--epochs', '-e', metavar='E', type=int, default=5, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', '-b', dest='batch_size', metavar='B', type=int, default=1, help='æ¯æ‰¹æ¬¡æ ·æœ¬æ•°')
    parser.add_argument('--learning-rate', '-l', metavar='LR', type=float, default=1e-5,
                        help='å­¦ä¹ çŽ‡', dest='lr')
    parser.add_argument('--load', '-f', type=str, default=False, help='ä»Ž.pthæ–‡ä»¶åŠ è½½æ¨¡åž‹')
    parser.add_argument('--scale', '-s', type=float, default=0.5, help='å›¾ç‰‡ç¼©æ”¾å› å­')
    parser.add_argument('--validation', '-v', dest='val', type=float, default=10.0,
                        help='éªŒè¯é›†æ¯”ä¾‹(0-100)')
    parser.add_argument('--amp', action='store_true', default=False, help='ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ')
    parser.add_argument('--bilinear', action='store_true', default=False, help='ä½¿ç”¨åŒçº¿æ€§ä¸Šé‡‡æ ·')
    parser.add_argument('--classes', '-c', type=int, default=2, help='ç±»åˆ«æ•°')

    return parser.parse_args()



# ================================
# ä¸»ç¨‹åºå…¥å£
# ================================
if __name__ == '__main__':
    """
    ä¸»ç¨‹åºå…¥å£ç‚¹ï¼Œè´Ÿè´£åˆå§‹åŒ–è®­ç»ƒçŽ¯å¢ƒå¹¶å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
    
    ä¸»è¦åŠŸèƒ½ï¼š
    1. è§£æžå‘½ä»¤è¡Œå‚æ•°
    2. åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
    3. æ£€æµ‹å’Œé…ç½®è®¡ç®—è®¾å¤‡
    4. åˆ›å»ºå’Œé…ç½®UNetæ¨¡åž‹
    5. åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹ï¼ˆå¯é€‰ï¼‰
    6. å¯åŠ¨è®­ç»ƒæµç¨‹
    7. å¤„ç†å¼‚å¸¸å’Œé”™è¯¯æ¢å¤
    
    å¼‚å¸¸å¤„ç†ï¼š
    - æ˜¾å­˜æº¢å‡ºï¼šè‡ªåŠ¨å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æœºåˆ¶
    - æ¨¡åž‹åŠ è½½å¤±è´¥ï¼šæä¾›æ¸…æ™°çš„é”™è¯¯ä¿¡æ¯
    - è®¾å¤‡é…ç½®é—®é¢˜ï¼šè‡ªåŠ¨å›žé€€åˆ°CPU
    """
    
    # ================================
    # 1. çŽ¯å¢ƒåˆå§‹åŒ–
    # ================================
    # è§£æžå‘½ä»¤è¡Œå‚æ•°ï¼ŒèŽ·å–è®­ç»ƒé…ç½®
    args = get_args()
    
    # é…ç½®æ—¥å¿—ç³»ç»Ÿ
    # ä½¿ç”¨INFOçº§åˆ«è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„é‡è¦ä¿¡æ¯
    logging.basicConfig(
        level=logging.INFO, 
        format='%(asctime)s - %(levelname)s: %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # è‡ªåŠ¨æ£€æµ‹å’Œé…ç½®è®¡ç®—è®¾å¤‡
    # ä¼˜å…ˆä½¿ç”¨GPUï¼Œå¦‚æžœä¸å¯ç”¨åˆ™å›žé€€åˆ°CPU
    if torch.cuda.is_available():
        device = torch.device('cuda')
        logging.info(f'æ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œä½¿ç”¨GPUè®­ç»ƒ: {torch.cuda.get_device_name()}')
        logging.info(f'GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB')
    elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
        device = torch.device('mps')
        logging.info('æ£€æµ‹åˆ°Apple Siliconè®¾å¤‡ï¼Œä½¿ç”¨MPSè®­ç»ƒ')
    else:
        device = torch.device('cpu')
        logging.warning('æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒï¼ˆé€Ÿåº¦è¾ƒæ…¢ï¼‰')
    
    logging.info(f'è®­ç»ƒè®¾å¤‡: {device}')

    # ================================
    # 2. æ¨¡åž‹åˆ›å»ºå’Œé…ç½®
    # ================================
    # åˆ›å»ºUNetæ¨¡åž‹å®žä¾‹
    # n_channels=3: RGBå›¾åƒè¾“å…¥é€šé“æ•°
    # n_classes: åˆ†å‰²ç±»åˆ«æ•°ï¼ˆåŒ…æ‹¬èƒŒæ™¯ç±»ï¼‰
    # bilinear: æ˜¯å¦ä½¿ç”¨åŒçº¿æ€§æ’å€¼ä¸Šé‡‡æ ·
    model = UNet(
        n_channels=3, 
        n_classes=args.classes, 
        bilinear=args.bilinear
    )
    
    # é…ç½®å†…å­˜æ ¼å¼ä¼˜åŒ–
    # channels_lastæ ¼å¼å¯ä»¥æå‡GPUæ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯åœ¨å·ç§¯æ“ä½œä¸­
    model = model.to(memory_format=torch.channels_last)

    # æ‰“å°æ¨¡åž‹ç»“æž„ä¿¡æ¯
    logging.info(f'''
    ================================
    æ¨¡åž‹é…ç½®ä¿¡æ¯
    ================================
    è¾“å…¥é€šé“æ•°:         {model.n_channels} (RGBå›¾åƒ)
    è¾“å‡ºé€šé“æ•°:         {model.n_classes} (åˆ†å‰²ç±»åˆ«)
    ä¸Šé‡‡æ ·æ–¹å¼:         {"åŒçº¿æ€§æ’å€¼" if model.bilinear else "è½¬ç½®å·ç§¯"}
    æ¨¡åž‹å‚æ•°é‡:         {sum(p.numel() for p in model.parameters()):,}
    å¯è®­ç»ƒå‚æ•°:         {sum(p.numel() for p in model.parameters() if p.requires_grad):,}
    ================================
    ''')

    # ================================
    # 3. é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½
    # ================================
    # å¦‚æžœæŒ‡å®šäº†é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„ï¼Œåˆ™åŠ è½½æ¨¡åž‹æƒé‡
    if args.load:
        try:
            logging.info(f'æ­£åœ¨ä»Ž {args.load} åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹...')
            
            # åŠ è½½æ£€æŸ¥ç‚¹æ–‡ä»¶
            state_dict = torch.load(args.load, map_location=device)
            
            # æ¸…ç†æ•°æ®é›†ç›¸å…³çš„å‚æ•°
            # mask_valuesæ˜¯æ•°æ®é›†ç‰¹å®šçš„ï¼Œä¸å±žäºŽæ¨¡åž‹å‚æ•°
            if 'mask_values' in state_dict:
                del state_dict['mask_values']
                logging.info('å·²ç§»é™¤æ•°æ®é›†ç›¸å…³çš„mask_valueså‚æ•°')
            
            # åŠ è½½æ¨¡åž‹å‚æ•°
            model.load_state_dict(state_dict, strict=False)
            logging.info('âœ… é¢„è®­ç»ƒæ¨¡åž‹åŠ è½½æˆåŠŸï¼')
            
            # æ˜¾ç¤ºåŠ è½½çš„æ£€æŸ¥ç‚¹ä¿¡æ¯
            if 'epoch' in state_dict:
                logging.info(f'æ£€æŸ¥ç‚¹è½®æ¬¡: {state_dict["epoch"]}')
            if 'loss' in state_dict:
                logging.info(f'æ£€æŸ¥ç‚¹æŸå¤±: {state_dict["loss"]:.4f}')
                
        except FileNotFoundError:
            logging.error(f'âŒ æ£€æŸ¥ç‚¹æ–‡ä»¶ä¸å­˜åœ¨: {args.load}')
            logging.info('è¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®')
            sys.exit(1)
        except Exception as e:
            logging.error(f'âŒ åŠ è½½é¢„è®­ç»ƒæ¨¡åž‹æ—¶å‡ºé”™: {str(e)}')
            logging.info('è¯·æ£€æŸ¥æ£€æŸ¥ç‚¹æ–‡ä»¶æ˜¯å¦å®Œæ•´æˆ–å…¼å®¹')
            sys.exit(1)

    # å°†æ¨¡åž‹ç§»åŠ¨åˆ°æŒ‡å®šè®¾å¤‡ï¼ˆGPU/CPUï¼‰
    model.to(device=device)
    logging.info(f'æ¨¡åž‹å·²ç§»åŠ¨åˆ°è®¾å¤‡: {device}')

    # ================================
    # 4. è®­ç»ƒæµç¨‹å¯åŠ¨
    # ================================
    try:
        logging.info('ðŸš€ å¼€å§‹è®­ç»ƒè¿‡ç¨‹...')
        
        # è°ƒç”¨ä¸»è®­ç»ƒå‡½æ•°
        train_model(
            model=model,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.lr,
            device=device,
            img_scale=args.scale,
            val_percent=args.val / 100,
            amp=args.amp
        )
        
        logging.info('ðŸŽ‰ è®­ç»ƒæˆåŠŸå®Œæˆï¼')
        
    except torch.cuda.OutOfMemoryError as e:
        # ================================
        # 5. æ˜¾å­˜æº¢å‡ºå¼‚å¸¸å¤„ç†
        # ================================
        logging.error('âš ï¸ æ£€æµ‹åˆ°GPUæ˜¾å­˜æº¢å‡ºï¼æ­£åœ¨é‡‡å–è¡¥æ•‘æŽªæ–½...')
        
        # æ¸…ç†CUDAç¼“å­˜ï¼Œé‡Šæ”¾æœªä½¿ç”¨çš„æ˜¾å­˜
        logging.info('1. æ¸…ç†CUDAç¼“å­˜...')
        torch.cuda.empty_cache()
        
        # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æœºåˆ¶
        # è¿™ä¼šç”¨è®¡ç®—æ—¶é—´æ¢å–æ˜¾å­˜ç©ºé—´ï¼Œé€‚ç”¨äºŽå¤§æ¨¡åž‹è®­ç»ƒ
        logging.info('2. å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹æœºåˆ¶(gradient checkpointing)...')
        model.use_checkpointing()
        
        # æä¾›ä¼˜åŒ–å»ºè®®
        logging.warning('''
        ================================
        æ˜¾å­˜ä¼˜åŒ–å»ºè®®
        ================================
        å½“å‰é…ç½®å¯èƒ½è¶…å‡ºGPUæ˜¾å­˜é™åˆ¶ï¼Œå»ºè®®ï¼š
        
        1. å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š
           python train.py --amp
        
        2. å‡å°æ‰¹æ¬¡å¤§å°ï¼š
           python train.py --batch-size 1
        
        3. å‡å°è¾“å…¥å›¾åƒå°ºå¯¸ï¼š
           python train.py --scale 0.25
        
        4. ä½¿ç”¨åŒçº¿æ€§ä¸Šé‡‡æ ·ï¼š
           python train.py --bilinear
        
        5. ç»„åˆä¼˜åŒ–ï¼š
           python train.py --amp --batch-size 1 --scale 0.25 --bilinear
        ================================
        ''')
        
        # é‡æ–°å°è¯•è®­ç»ƒ
        logging.info('3. ä½¿ç”¨ä¼˜åŒ–åŽçš„è®¾ç½®é‡æ–°è®­ç»ƒ...')
        train_model(
            model=model,
            epochs=args.epochs,
            batch_size=max(1, args.batch_size // 2),  # å‡å°æ‰¹æ¬¡å¤§å°
            learning_rate=args.lr,
            device=device,
            img_scale=min(0.25, args.scale),  # å‡å°å›¾åƒå°ºå¯¸
            val_percent=args.val / 100,
            amp=True  # å¼ºåˆ¶å¯ç”¨æ··åˆç²¾åº¦
        )
        
    except KeyboardInterrupt:
        logging.info('â¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­')
        logging.info('ðŸ’¡ æç¤ºï¼šå¯ä»¥ä»Žæœ€æ–°ä¿å­˜çš„æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ')
        
    except Exception as e:
        logging.error(f'âŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {str(e)}')
        logging.error('è¯·æ£€æŸ¥æ•°æ®è·¯å¾„ã€æ¨¡åž‹é…ç½®å’Œç³»ç»ŸçŽ¯å¢ƒ')
        raise e
    
    finally:
        # æ¸…ç†èµ„æº
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logging.info('ðŸ”§ èµ„æºæ¸…ç†å®Œæˆ')
